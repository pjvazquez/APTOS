{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# anulo el uso de la GPU por falta de memoria\n",
    "import os, sys\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "else:\n",
    "    print(\"No GPU found\")\n",
    "\n",
    "# No GPU found\n",
    "\n",
    "# tf.enable_eager_execution()\n",
    "tf.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from PIL import Image, ImageOps\n",
    "import cv2\n",
    "\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from keras.models import Sequential\n",
    "from keras.layers import InputLayer\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "import keras.backend as K\n",
    "\n",
    "from sklearn.metrics import f1_score, fbeta_score\n",
    "from keras.utils import Sequence\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "WORKERS = 2\n",
    "CHANNEL = 3\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "IMG_SIZE = 512\n",
    "NUM_CLASSES = 5\n",
    "SEED = 77\n",
    "TRAIN_NUM = 1000 # use 1000 when you just want to explore new idea, use -1 for full train\n",
    "\n",
    "\n",
    "DATA_PATH = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['processed_train_images', 'train.csv', 'train_images', 'test.csv', 'test_images', 'processed_test_images', 'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 3407234163082605366\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 7433479149977161742\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction. Explore first, train later.\n",
    "\n",
    "Hi everyone! As *Aravind Eye Hospital* is one of my favorite organization in the world; they take care of poor people's eyes for free with an impressive sustainable business model.  I will try my best to contribute something to our community. One intuitive way to improve the performance of our model is to simply improve the quality of input images. In this kernel, I will share two ideas which I hope may be useful to some of you : \n",
    "\n",
    "- **Decolorize images** : here as we will see, images come with many different lighting conditions, some images are quite dark and difficult to visualize. We can try to convert the image to gray scale, and visualize better. Alternatively, we can try the method of [Ben Graham (last competition's winner)](https://github.com/btgraham/SparseConvNet/tree/kaggle_Diabetic_Retinopathy_competition)\n",
    "- **Cropping uninformative area** : everyone know this :) Here, I just find the codes from internet and choose the best one for you :)\n",
    "\n",
    "We are going to apply both techniques to both the official data, and the past competition data (shout out @tanlikesmath for creating this dataset! https://www.kaggle.com/tanlikesmath/diabetic-retinopathy-resized . In the updated version, I also try @donkeys' dataset https://www.kaggle.com/donkeys/retinopathy-train-2015 , which is .png which may be have higer image quality than .jpeg format)\n",
    "\n",
    "If I found more useful tricks, I will update the notebook, or if you have more useful tricks and would love to share, please let me know!\n",
    "\n",
    "I use some parts of codes from @mathormad and @artgor kernels. Thanks both of you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us start by loading the train/test dataframes. The `train_test_split` here is in fact not necessary. But when I first fork the kernel from @mathormad, I found some interesting examples using this split and the current `SEED`, so I continue to use them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(DATA_PATH + '/train.csv')\n",
    "df_test = pd.read_csv(DATA_PATH + '/test.csv')\n",
    "\n",
    "x = df_train['id_code']\n",
    "y = df_train['diagnosis']\n",
    "\n",
    "x, y = shuffle(x, y, random_state=SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3112,) (3112,) (550,) (550,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7efd718e2400>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_x, valid_x, train_y, valid_y = train_test_split(x, y, test_size=0.15,\n",
    "                                                      stratify=y, random_state=SEED)\n",
    "print(train_x.shape, train_y.shape, valid_x.shape, valid_y.shape)\n",
    "train_y.hist()\n",
    "valid_y.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Simple picture to explain Diabetic Retinopathy\n",
    "\n",
    "How do we know that a patient have diabetic retinopahy? There are 5 things to spot on. Image credit https://www.eyeops.com/\n",
    "![credit : https://www.eyeops.com/](https://sa1s3optim.patientpop.com/assets/images/provider/photos/1947516.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From quick investigations of the data (see various pictures below), I found that *Hemorrphages, Hard Exudates and Cotton Wool spots* are quite easily observed. However, I still could not find examples of *Aneurysm* or *Abnormal Growth of Blood Vessels* from our data yet. Perhaps the latter two cases are important if we want to catch up human benchmnark using our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Aquí transcribimos las imágenes a un fichero tipo TFRECORD de tamaño\n",
    "width = 400\n",
    "height = 500\n",
    "\n",
    "size = (width,height)\n",
    "# the TFRecord file\n",
    "# imagesXXX_XXX file has three elements in feature definition\n",
    "# images2XXX_XXX has two elements in feature definition\n",
    "TFRecord_filename = '/mnt/DATA-SSD/DataSandbox/images2{}_{}.tfrecords'.format(width,height)\n",
    "\n",
    "raw_image_dataset = tf.data.TFRecordDataset(TFRecord_filename)\n",
    "'''\n",
    "image_feature_description = {\n",
    "    'img': tf.io.FixedLenFeature([200000], tf.float32),\n",
    "    'label': tf.io.FixedLenFeature([5], tf.float32),\n",
    "    # https://stackoverflow.com/questions/54440226/using-tfrecords-with-keras/54440703\n",
    "    # no need to set it if it has a single dimension\n",
    "    'name': tf.io.FixedLenFeature([], tf.string)\n",
    "}\n",
    "'''\n",
    "\n",
    "image_feature_description = {\n",
    "    'img': tf.io.FixedLenFeature([200000], tf.float32),\n",
    "    'label': tf.io.FixedLenFeature([5], tf.float32)\n",
    "}\n",
    "\n",
    "\n",
    "# aqui parseamos el fichero bajo demanda\n",
    "# y lo devolvemos con el formato esperado\n",
    "\n",
    "def _parse_image_function(example_proto):\n",
    "    # print('_parse_image_function')\n",
    "    print(example_proto)\n",
    "    # Parse the input tf.Example proto using the dictionary above.\n",
    "    features = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "    print('Shape of image = {} and of label = {} '.\\\n",
    "          format(features['img'].shape, features['label'].shape))\n",
    "    return tf.math.scalar_mul(0.00392157,tf.reshape(features['img'],(500,400,1))) ,features['label']\n",
    "\n",
    "\n",
    "# parsed_image_dataset = raw_image_dataset.map(_parse_image_function)\n",
    "\n",
    "# iterator = parsed_image_dataset.make_one_shot_iterator()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aqui la funcion auxiliar que genera un iterador para el fichero\n",
    "\n",
    "def load_dataset(input_path, batch_size, shuffle_buffer):\n",
    "    dataset = tf.data.TFRecordDataset(input_path)\n",
    "    dataset = dataset.shuffle(shuffle_buffer).repeat()  # shuffle and repeat\n",
    "    dataset = dataset.map(_parse_image_function)\n",
    "    dataset = dataset.batch(batch_size).prefetch(1)  # batch and prefetch\n",
    "\n",
    "    return dataset.make_one_shot_iterator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"arg0:0\", shape=(), dtype=string)\n",
      "Shape of image = (200000,) and of label = (5,) \n"
     ]
    }
   ],
   "source": [
    "# aquí generamos el iterador\n",
    "# ojo, esto lo tengo que controlar para que me recorra todo el dataset\n",
    "input_path = TFRecord_filename\n",
    "batch_size = 1\n",
    "shuffle_buffer = 1\n",
    "train_iterator = load_dataset(input_path, batch_size, shuffle_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 500, 400, 1)\n",
      "(?, 5)\n",
      "Tensor(\"IteratorGetNext:1\", shape=(?, 5), dtype=float32)\n",
      "Tensor(\"IteratorGetNext:0\", shape=(?, 500, 400, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# test iterator to see what is going on\n",
    "\n",
    "img_test, label_test = train_iterator.get_next()\n",
    "print(img_test.shape )\n",
    "print(label_test.shape)\n",
    "print(label_test)\n",
    "print(img_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "## inspired in real life\n",
    "\n",
    "help obtained from\n",
    "https://machinelearningmastery.com/keras-functional-api-deep-learning/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "# config.gpu_options.allow_growth = True\n",
    "# gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "\n",
    "# session = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_layer:  (?, 500, 400, 1)\n",
      "WARNING:tensorflow:From /home/pjvazquez/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "layer:  (?, 496, 396, 1000)\n",
      "layer:  (?, 99, 79, 1000)\n",
      "layer:  (?, 98, 78, 500)\n",
      "layer:  (?, 49, 39, 500)\n",
      "layer:  (?, 48, 38, 100)\n",
      "layer:  (?, 24, 19, 100)\n",
      "layer:  (?, 45600)\n",
      "layer:  (?, 50)\n",
      "output_layer:  (?, 5)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "# OPTION 1\n",
    "# ---------------------\n",
    "''' '''\n",
    "\n",
    "input_layer = tf.keras.Input(shape=(500,400,1))\n",
    "print('input_layer: ', input_layer.shape)\n",
    "\n",
    "layer1 = tf.keras.layers.Conv2D(1000, kernel_size=(5, 5), activation='relu')(input_layer)\n",
    "print('layer: ', layer1.shape)\n",
    "\n",
    "layer2 = tf.keras.layers.MaxPooling2D(pool_size=(5, 5))(layer1)\n",
    "print('layer: ', layer2.shape)\n",
    "\n",
    "layer3 = tf.keras.layers.Conv2D(500, kernel_size=(2, 2), activation='relu')(layer2)\n",
    "print('layer: ', layer3.shape)\n",
    "\n",
    "layer4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(layer3)\n",
    "print('layer: ', layer4.shape)\n",
    "\n",
    "layer5 = tf.keras.layers.Conv2D(100, kernel_size=(2, 2), activation='relu')(layer4)\n",
    "print('layer: ', layer5.shape)\n",
    "\n",
    "layer6 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(layer5)\n",
    "print('layer: ', layer6.shape)\n",
    "\n",
    "layer7 = tf.keras.layers.Flatten()(layer6)\n",
    "print('layer: ', layer7.shape)\n",
    "\n",
    "layer8 = tf.keras.layers.Dense(10*NUM_CLASSES, activation='relu')(layer7)\n",
    "print('layer: ', layer8.shape)\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')(layer8)\n",
    "print('output_layer: ', output_layer.shape)\n",
    "\n",
    "model = tf.keras.Model(inputs = [input_layer], outputs = [output_layer])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\n\\n\\nmodel = Sequential()\\n\\nmodel.add(InputLayer((500,400,1)))\\nmodel.add(Conv2D(1000, kernel_size=(5, 5), activation='relu'))\\nmodel.add(MaxPooling2D(pool_size=(5, 5)))\\nmodel.add(Conv2D(500, kernel_size=(2, 2), activation='relu'))\\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\\nmodel.add(Conv2D(100, kernel_size=(2, 2), activation='relu'))\\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\\nmodel.add(Flatten())\\nmodel.add(Dense(10*NUM_CLASSES, activation='relu'))\\nmodel.add(Dense(NUM_CLASSES, activation='softmax'))\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------\n",
    "# OPTION 2\n",
    "# ---------------------\n",
    "\n",
    "''' \n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(InputLayer((500,400,1)))\n",
    "model.add(Conv2D(1000, kernel_size=(5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(5, 5)))\n",
    "model.add(Conv2D(500, kernel_size=(2, 2), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(100, kernel_size=(2, 2), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10*NUM_CLASSES, activation='relu'))\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 500, 400, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 496, 396, 1000)    26000     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 99, 79, 1000)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 98, 78, 500)       2000500   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 49, 39, 500)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 48, 38, 100)       200100    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 24, 19, 100)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 45600)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                2280050   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 4,506,905\n",
      "Trainable params: 4,506,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# import pydot\n",
    "# import graphviz\n",
    "\n",
    "print(model.summary())\n",
    "# no va por alguna chuminada\n",
    "# tf.keras.utils.plot_model(model, 'model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pjvazquez/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "              loss = tf.keras.losses.CategoricalCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "logits and labels must have the same first dimension, got logits shape [1,5] and labels shape [5]\n\t [[{{node loss/dense_1_loss/CategoricalCrossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-41edca0232e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m                           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                           callbacks = [tf.keras.callbacks.ModelCheckpoint(model_file, verbose=1)])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m           \u001b[0;31m# `ins` can be callable in DistributionStrategy + eager case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m           \u001b[0mactual_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mins\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m           logging.warning('Your dataset iterator ran out of data; '\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: logits and labels must have the same first dimension, got logits shape [1,5] and labels shape [5]\n\t [[{{node loss/dense_1_loss/CategoricalCrossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "# este es el modelo estándar, con la entrada y la salida esperadas\n",
    "# hay que sustituirlo por un iterador.\n",
    "\n",
    "\n",
    "# No olvidarse de incluir un train_history para recuperar los datos del entrenamiento\n",
    "# 5000 epochs es una locura y el número de steps per epoch es muy bajo (no se recorren todas las imagenes)\n",
    "# el mínimo steps_per_epoch debería de ser total_muestras/batch_size\n",
    "\n",
    "lt = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "model_file = \"/mnt/DATA-SSD/DataSandbox/model{epoch:04d}.h5\"\n",
    "\n",
    "# with callback\n",
    "train_history = model.fit(train_iterator,\n",
    "                          epochs=500,\n",
    "                          steps_per_epoch=400,\n",
    "                          verbose=1,\n",
    "                          callbacks = [tf.keras.callbacks.ModelCheckpoint(model_file, verbose=1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
