{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anulo el uso de la GPU por falta de memoria\n",
    "import os, sys\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' #'-1' for no GPU\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "else:\n",
    "    print(\"No GPU found\")\n",
    "\n",
    "# No GPU found\n",
    "\n",
    "# tf.enable_eager_execution()\n",
    "tf.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from PIL import Image, ImageOps\n",
    "import cv2\n",
    "\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Sequential\n",
    "from keras.layers import InputLayer\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "import keras.backend as K\n",
    "\n",
    "from sklearn.metrics import f1_score, fbeta_score\n",
    "from keras.utils import Sequence\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "WORKERS = 2\n",
    "CHANNEL = 3\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "IMG_SIZE = 512\n",
    "NUM_CLASSES = 5\n",
    "SEED = 77\n",
    "TRAIN_NUM = 1000 # use 1000 when you just want to explore new idea, use -1 for full train\n",
    "\n",
    "\n",
    "DATA_PATH = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir(\"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction. Explore first, train later.\n",
    "\n",
    "Hi everyone! As *Aravind Eye Hospital* is one of my favorite organization in the world; they take care of poor people's eyes for free with an impressive sustainable business model.  I will try my best to contribute something to our community. One intuitive way to improve the performance of our model is to simply improve the quality of input images. In this kernel, I will share two ideas which I hope may be useful to some of you : \n",
    "\n",
    "- **Decolorize images** : here as we will see, images come with many different lighting conditions, some images are quite dark and difficult to visualize. We can try to convert the image to gray scale, and visualize better. Alternatively, we can try the method of [Ben Graham (last competition's winner)](https://github.com/btgraham/SparseConvNet/tree/kaggle_Diabetic_Retinopathy_competition)\n",
    "- **Cropping uninformative area** : everyone know this :) Here, I just find the codes from internet and choose the best one for you :)\n",
    "\n",
    "We are going to apply both techniques to both the official data, and the past competition data (shout out @tanlikesmath for creating this dataset! https://www.kaggle.com/tanlikesmath/diabetic-retinopathy-resized . In the updated version, I also try @donkeys' dataset https://www.kaggle.com/donkeys/retinopathy-train-2015 , which is .png which may be have higer image quality than .jpeg format)\n",
    "\n",
    "If I found more useful tricks, I will update the notebook, or if you have more useful tricks and would love to share, please let me know!\n",
    "\n",
    "I use some parts of codes from @mathormad and @artgor kernels. Thanks both of you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us start by loading the train/test dataframes. The `train_test_split` here is in fact not necessary. But when I first fork the kernel from @mathormad, I found some interesting examples using this split and the current `SEED`, so I continue to use them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(DATA_PATH + '/train.csv')\n",
    "df_test = pd.read_csv(DATA_PATH + '/test.csv')\n",
    "\n",
    "x = df_train['id_code']\n",
    "y = df_train['diagnosis']\n",
    "\n",
    "x, y = shuffle(x, y, random_state=SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(x.shape,  y.shape)\n",
    "y.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.groupby('diagnosis').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Simple picture to explain Diabetic Retinopathy\n",
    "\n",
    "How do we know that a patient have diabetic retinopahy? There are 5 things to spot on. Image credit https://www.eyeops.com/\n",
    "![credit : https://www.eyeops.com/](https://sa1s3optim.patientpop.com/assets/images/provider/photos/1947516.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From quick investigations of the data (see various pictures below), I found that *Hemorrphages, Hard Exudates and Cotton Wool spots* are quite easily observed. However, I still could not find examples of *Aneurysm* or *Abnormal Growth of Blood Vessels* from our data yet. Perhaps the latter two cases are important if we want to catch up human benchmnark using our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image1(img,tol=8):\n",
    "    # img is image data\n",
    "    # tol  is tolerance\n",
    "        \n",
    "    mask = img>tol\n",
    "    return img[np.ix_(mask.any(1),mask.any(0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance(image, clip_limit=3, GaussFilter = True, size=(400,500)):\n",
    "    # convert image to LAB color model\n",
    "    \n",
    "    image_lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    # split the image into L, A, and B channels\n",
    "    l_channel, a_channel, b_channel = cv2.split(image_lab)\n",
    "\n",
    "    # apply CLAHE to lightness channel\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=(8, 8))\n",
    "    cl = clahe.apply(l_channel)\n",
    "\n",
    "    # merge the CLAHE enhanced L channel with the original A and B channel\n",
    "    merged_channels = cv2.merge((cl, a_channel, b_channel))\n",
    "\n",
    "    # convert iamge from LAB color model back to RGB color model\n",
    "    image = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)\n",
    "    image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Auto Crop image\n",
    "    cropped_image = crop_image1(image_gray, 8)\n",
    "    \n",
    "    # resize image\n",
    "    resized_image = cv2.resize(cropped_image, size)\n",
    "    # Include Ben Graham preprocessing (-4 value)\n",
    "    if GaussFilter:\n",
    "        final_image=cv2.addWeighted ( resized_image,4, \n",
    "                                     cv2.GaussianBlur( resized_image , (0,0) , IMG_SIZE/10) ,-4 ,128)\n",
    "        return final_image\n",
    "    else:\n",
    "        return resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "# Aquí transcribimos las imágenes a un fichero tipo TFRECORD de tamaño\n",
    "# el primer modelo usaba imágenes de 100x125\n",
    "width = 200 # 400\n",
    "height = 250 # 500\n",
    "\n",
    "size = (width,height)\n",
    "\n",
    "image_x = []\n",
    "\n",
    "for i in tqdm.tqdm(x):\n",
    "    path=f\"/mnt/DATA-SSD/DataSandbox/APTOS/train_images/{i}.png\"\n",
    "    image = cv2.imread(path)\n",
    "    enhanced_image = enhance(image,2, False, size)\n",
    "    # img = image.img_to_array(enhanced_image)\n",
    "    img = enhanced_image.reshape((width, height, 1))/255\n",
    "    image_x.append(img)\n",
    "\n",
    "X = np.array(image_x)\n",
    "\n",
    "Y = to_categorical(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x), X.shape)\n",
    "print(len(Y), Y.shape)\n",
    "print(np.amax(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "## inspired in real life\n",
    "\n",
    "help obtained from\n",
    "https://machinelearningmastery.com/keras-functional-api-deep-learning/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# OPTION 3\n",
    "# model with two processing routes with different receptive fields size (kernel sizes)\n",
    "# ---------------------\n",
    "''' '''\n",
    "\n",
    "input_layer = tf.keras.Input(shape=(width,height,1))\n",
    "print('input_layer: ', input_layer.shape)\n",
    "\n",
    "layer1a = tf.keras.layers.Conv2D(128, kernel_size=(1, 1), activation='relu')(input_layer)\n",
    "print('layer: ', layer1a.shape)\n",
    "\n",
    "layer1b = tf.keras.layers.Conv2D(128, kernel_size=(10, 10), activation='relu')(input_layer)\n",
    "print('layer: ', layer1b.shape)\n",
    "\n",
    "layer2a = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu')(layer1a)\n",
    "print('layer: ', layer2a.shape)\n",
    "\n",
    "layer2b = tf.keras.layers.Conv2D(64, kernel_size=(10, 10), activation='relu')(layer1b)\n",
    "print('layer: ', layer2b.shape)\n",
    "\n",
    "layer3a = tf.keras.layers.Conv2D(32, kernel_size=(2, 2), activation='relu')(layer2a)\n",
    "print('layer: ', layer3a.shape)\n",
    "\n",
    "layer3b = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu')(layer2b)\n",
    "print('layer: ', layer3b.shape)\n",
    "\n",
    "layer4a = tf.keras.layers.MaxPooling2D(pool_size=(3, 3))(layer3a)\n",
    "print('layer: ', layer3a.shape)\n",
    "\n",
    "layer4b = tf.keras.layers.MaxPooling2D(pool_size=(3, 3))(layer3b)\n",
    "print('layer: ', layer3b.shape)\n",
    "\n",
    "layer5a = tf.keras.layers.Flatten()(layer4a)\n",
    "print('layer: ', layer4a.shape)\n",
    "\n",
    "layer5b = tf.keras.layers.Flatten()(layer4b)\n",
    "print('layer: ', layer4b.shape)\n",
    "\n",
    "layer6 = tf.keras.layers.concatenate([layer5a, layer5b])\n",
    "\n",
    "layer7 = tf.keras.layers.Dense(10*NUM_CLASSES, activation='relu')(layer6)\n",
    "print('layer: ', layer6.shape)\n",
    "\n",
    "layer8 = tf.keras.layers.Dense(5*NUM_CLASSES, activation='relu')(layer7)\n",
    "print('layer: ', layer6.shape)\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')(layer8)\n",
    "print('output_layer: ', output_layer.shape)\n",
    "\n",
    "model = tf.keras.Model(inputs = [input_layer], outputs = [output_layer])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pydot\n",
    "# import graphviz\n",
    "\n",
    "print(model.summary())\n",
    "# no va por alguna chuminada\n",
    "# tf.keras.utils.plot_model(model, 'model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "#              loss = tf.keras.losses.CategoricalCrossentropy())\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# este es el modelo estándar, con la entrada y la salida esperadas\n",
    "# hay que sustituirlo por un iterador.\n",
    "\n",
    "\n",
    "# No olvidarse de incluir un train_history para recuperar los datos del entrenamiento\n",
    "# 5000 epochs es una locura y el número de steps per epoch es muy bajo (no se recorren todas las imagenes)\n",
    "# el mínimo steps_per_epoch debería de ser total_muestras/batch_size\n",
    "\n",
    "lt = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "model_file = \"/mnt/DATA-SSD/DataSandbox/APTOS/model_\" + lt + \"_{epoch:04d}.h5\"\n",
    "\n",
    "# with callback\n",
    "train_history = model.fit(X, Y,\n",
    "                          batch_size = 10,\n",
    "                          epochs=2000,\n",
    "                          verbose=1,\n",
    "                          shuffle = True,\n",
    "                          callbacks = [tf.keras.callbacks.ModelCheckpoint(model_file, \n",
    "                                                                          save_weights_only = False,\n",
    "                                                                          verbose=1,\n",
    "                                                                          period=50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(train_history.history['acc'])\n",
    "plt.plot(train_history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(train_history.history['loss'])\n",
    "plt.plot(train_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = {}\n",
    "for i in tqdm.tqdm(df_train.iterrows()):\n",
    "    path=f\"/mnt/DATA-SSD/DataSandbox/APTOS/train_images/{i[1][0]}.png\"\n",
    "    image = cv2.imread(path)\n",
    "    enhanced_image = enhance(image,2, True, size)\n",
    "    # img = image.img_to_array(enhanced_image)\n",
    "    img = enhanced_image.reshape((1,width, height, 1))/255\n",
    "    results_train[i[0]] = [i[1][0], i[1][1], model.predict(img)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame.from_dict(results_train, \n",
    "                                    orient='index', \n",
    "                                    columns=['imge_id', 'diagnoses', 'prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnoses_from_list(row):\n",
    "    n = 0.0\n",
    "    diagnoses = 0.0\n",
    "    for i in row[0]:\n",
    "        diagnoses += i*n\n",
    "        n += 1.0\n",
    "    return int(diagnoses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(results_df.iloc[0][2])\n",
    "# print(diagnoses_from_list(results_df.prediction[0]))\n",
    "results_df['predicted_diagnoses'] = results_df.apply( lambda x: diagnoses_from_list(x.prediction), axis=1)\n",
    "results_df['predict_ok'] = results_df.apply( lambda x: 1 if x.predicted_diagnoses==x.diagnoses else 0, axis=1)\n",
    "results_df['predict_error'] = results_df.apply( lambda x: 1 if x.predicted_diagnoses!=x.diagnoses else 0, axis=1)\n",
    "results_df['prediction_error'] = results_df['diagnoses'].sub(results_df['predicted_diagnoses'])\n",
    "results_df.sum(axis=0)\n",
    "# results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby('diagnoses').count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby('diagnoses').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "results_df.to_pickle(f\"/mnt/DATA-SSD/DataSandbox/APTOS/results_df_{lt}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
