{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# anulo el uso de la GPU por falta de memoria\n",
    "import os, sys\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "else:\n",
    "    print(\"No GPU found\")\n",
    "\n",
    "# No GPU found\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "tf.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from PIL import Image, ImageOps\n",
    "import cv2\n",
    "\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "import keras.backend as K\n",
    "\n",
    "from sklearn.metrics import f1_score, fbeta_score\n",
    "from keras.utils import Sequence\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "WORKERS = 2\n",
    "CHANNEL = 3\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "IMG_SIZE = 512\n",
    "NUM_CLASSES = 5\n",
    "SEED = 77\n",
    "TRAIN_NUM = 1000 # use 1000 when you just want to explore new idea, use -1 for full train\n",
    "\n",
    "\n",
    "DATA_PATH = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['processed_train_images', 'train.csv', 'train_images', 'test.csv', 'test_images', 'processed_test_images', 'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 11050065301690295522\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 13278362082181541916\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction. Explore first, train later.\n",
    "\n",
    "Hi everyone! As *Aravind Eye Hospital* is one of my favorite organization in the world; they take care of poor people's eyes for free with an impressive sustainable business model.  I will try my best to contribute something to our community. One intuitive way to improve the performance of our model is to simply improve the quality of input images. In this kernel, I will share two ideas which I hope may be useful to some of you : \n",
    "\n",
    "- **Decolorize images** : here as we will see, images come with many different lighting conditions, some images are quite dark and difficult to visualize. We can try to convert the image to gray scale, and visualize better. Alternatively, we can try the method of [Ben Graham (last competition's winner)](https://github.com/btgraham/SparseConvNet/tree/kaggle_Diabetic_Retinopathy_competition)\n",
    "- **Cropping uninformative area** : everyone know this :) Here, I just find the codes from internet and choose the best one for you :)\n",
    "\n",
    "We are going to apply both techniques to both the official data, and the past competition data (shout out @tanlikesmath for creating this dataset! https://www.kaggle.com/tanlikesmath/diabetic-retinopathy-resized . In the updated version, I also try @donkeys' dataset https://www.kaggle.com/donkeys/retinopathy-train-2015 , which is .png which may be have higer image quality than .jpeg format)\n",
    "\n",
    "If I found more useful tricks, I will update the notebook, or if you have more useful tricks and would love to share, please let me know!\n",
    "\n",
    "I use some parts of codes from @mathormad and @artgor kernels. Thanks both of you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us start by loading the train/test dataframes. The `train_test_split` here is in fact not necessary. But when I first fork the kernel from @mathormad, I found some interesting examples using this split and the current `SEED`, so I continue to use them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(DATA_PATH + '/train.csv')\n",
    "df_test = pd.read_csv(DATA_PATH + '/test.csv')\n",
    "\n",
    "x = df_train['id_code']\n",
    "y = df_train['diagnosis']\n",
    "\n",
    "x, y = shuffle(x, y, random_state=SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3112,) (3112,) (550,) (550,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f48a882c390>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_x, valid_x, train_y, valid_y = train_test_split(x, y, test_size=0.15,\n",
    "                                                      stratify=y, random_state=SEED)\n",
    "print(train_x.shape, train_y.shape, valid_x.shape, valid_y.shape)\n",
    "train_y.hist()\n",
    "valid_y.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Simple picture to explain Diabetic Retinopathy\n",
    "\n",
    "How do we know that a patient have diabetic retinopahy? There are 5 things to spot on. Image credit https://www.eyeops.com/\n",
    "![credit : https://www.eyeops.com/](https://sa1s3optim.patientpop.com/assets/images/provider/photos/1947516.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From quick investigations of the data (see various pictures below), I found that *Hemorrphages, Hard Exudates and Cotton Wool spots* are quite easily observed. However, I still could not find examples of *Aneurysm* or *Abnormal Growth of Blood Vessels* from our data yet. Perhaps the latter two cases are important if we want to catch up human benchmnark using our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"arg0:0\", shape=(), dtype=string)\n",
      "Shape of image = (200000,) and of label = (5,) and of name = (1,)\n",
      "WARNING:tensorflow:From /home/pjvazquez/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Aquí transcribimos las imágenes a un fichero tipo TFRECORD de tamaño\n",
    "width = 400\n",
    "height = 500\n",
    "\n",
    "size = (width,height)\n",
    "# the TFRecord file\n",
    "TFRecord_filename = '/mnt/DATA-SSD/DataSandbox/images{}_{}.tfrecords'.format(width,height)\n",
    "\n",
    "raw_image_dataset = tf.data.TFRecordDataset(TFRecord_filename)\n",
    "\n",
    "image_feature_description = {\n",
    "    'img': tf.io.FixedLenFeature([200000], tf.float32),\n",
    "    'label': tf.io.FixedLenFeature([5], tf.float32),\n",
    "    'name': tf.io.FixedLenFeature([1], tf.string)\n",
    "}\n",
    "\n",
    "\n",
    "# aqui parseamos el fichero bajo demanda\n",
    "# y lo devolvemos con el formato esperado\n",
    "\n",
    "def _parse_image_function(example_proto):\n",
    "    # print('_parse_image_function')\n",
    "    print(example_proto)\n",
    "    # Parse the input tf.Example proto using the dictionary above.\n",
    "    features = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "    print('Shape of image = {} and of label = {} and of name = {}'.format(features['img'].shape, features['label'].shape, features['name'].shape))\n",
    "    return tf.math.scalar_mul(0.00392157,tf.reshape(features['img'],(500,400,1))), tf.reshape(features['label'],(5,1))\n",
    "\n",
    "\n",
    "parsed_image_dataset = raw_image_dataset.map(_parse_image_function)\n",
    "\n",
    "iterator = parsed_image_dataset.make_one_shot_iterator()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 400, 1)\n",
      "(5, 1)\n",
      "tf.Tensor(0.83921593, shape=(), dtype=float32)\n",
      "tf.Tensor(56331.46, shape=(), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]], shape=(5, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# test iterator to see what is going on\n",
    "img_test, label_test = iterator.get_next()\n",
    "\n",
    "print(img_test.shape)\n",
    "print(label_test.shape)\n",
    "# print(name.shape)\n",
    "print(tf.math.reduce_max(img_test))\n",
    "print(tf.math.reduce_sum(img_test))\n",
    "\n",
    "# plt.imshow(img_test, cmap='gray')\n",
    "print(label_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor shapes:  (TensorShape([Dimension(500), Dimension(400), Dimension(1)]), TensorShape([Dimension(5), Dimension(1)]))\n",
      "tensor clases:  (<class 'tensorflow.python.framework.ops.Tensor'>, <class 'tensorflow.python.framework.ops.Tensor'>)\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = int(parsed_image_dataset.output_shapes[1][0])\n",
    "INPUT_SHAPE =  parsed_image_dataset.output_shapes[0]\n",
    "print('tensor shapes: ', parsed_image_dataset.output_shapes)\n",
    "print('tensor clases: ', parsed_image_dataset.output_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aqui la funcion auxiliar que genera un iterador para el fichero\n",
    "def load_dataset(input_path, batch_size, shuffle_buffer):\n",
    "    print('load_dataset')\n",
    "    dataset = tf.data.TFRecordDataset(input_path)\n",
    "    dataset = dataset.shuffle(shuffle_buffer).repeat()  # shuffle and repeat\n",
    "    dataset = dataset.map(_parse_image_function, num_parallel_calls=16)\n",
    "    dataset = dataset.batch(batch_size).prefetch(1)  # batch and prefetch\n",
    "\n",
    "    return dataset.make_one_shot_iterator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_dataset\n",
      "Tensor(\"arg0:0\", shape=(), dtype=string)\n",
      "Shape of image = (200000,) and of label = (5,) and of name = (1,)\n"
     ]
    }
   ],
   "source": [
    "# aquí generamos el iterador\n",
    "# ojo, esto lo tengo que controlar para que me recorra todo el dataset\n",
    "input_path = TFRecord_filename\n",
    "batch_size = 1\n",
    "shuffle_buffer = 10\n",
    "train_iterator = load_dataset(input_path, batch_size, shuffle_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 500, 400, 1)\n",
      "(1, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "# test iterator to see what is going on\n",
    "\n",
    "img_test, label_test = train_iterator.get_next()\n",
    "print(img_test.shape )\n",
    "print(label_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "## inspired in real life\n",
    "\n",
    "help obtained from\n",
    "https://machinelearningmastery.com/keras-functional-api-deep-learning/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "# config.gpu_options.allow_growth = True\n",
    "# gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "# session = tf.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.keras.Input(shape=(500,400,1))\n",
    "layer = tf.keras.layers.Conv2D(10000, kernel_size=(5, 5), activation='relu')(input_layer)\n",
    "layer = tf.keras.layers.MaxPooling2D(pool_size=(3, 3))(layer)\n",
    "layer = tf.keras.layers.Conv2D(5000, kernel_size=(2, 2), activation='relu')(layer)\n",
    "layer = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(layer)\n",
    "layer = tf.keras.layers.Conv2D(100, kernel_size=(2, 2), activation='relu')(layer)\n",
    "layer = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(layer)\n",
    "layer = tf.keras.layers.Flatten()(layer)\n",
    "layer = tf.keras.layers.Dense(NUM_CLASSES, activation='relu')(layer)\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')(layer)\n",
    "\n",
    "model = tf.keras.Model(inputs = input_layer, outputs = output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 500, 400, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 496, 396, 10000)   260000    \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 165, 132, 10000)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 164, 131, 5000)    200005000 \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 82, 65, 5000)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 81, 64, 100)       2000100   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 40, 32, 100)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 640005    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 30        \n",
      "=================================================================\n",
      "Total params: 202,905,135\n",
      "Trainable params: 202,905,135\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())\n",
    "## no va por alguna chuminada\n",
    "## tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "              loss='categorical_crossentropy',\n",
    "             sample_weight_mode=\"temporal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "WARNING:tensorflow:From /home/pjvazquez/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-eb0351988e8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m                           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                           verbose=1)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m           initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    852\u001b[0m     elif distributed_training_utils.is_tpu_strategy(\n\u001b[1;32m    853\u001b[0m         self._distribution_strategy):\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m       \u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1189\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_fit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3164\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3165\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3166\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3167\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n\u001b[1;32m   3168\u001b[0m                                  [x.numpy() for x in outputs])\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    366\u001b[0m           \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Got two values for keyword '{}'.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munused_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Keyword arguments {} unknown.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    267\u001b[0m           \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m           \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction_call_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_proto_serialized\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m           executor_type=function_call_options.executor_type)\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/functional_ops.py\u001b[0m in \u001b[0;36mpartitioned_call\u001b[0;34m(args, f, tout, executing_eagerly, config, executor_type)\u001b[0m\n\u001b[1;32m   1081\u001b[0m       outputs = gen_functional_ops.stateful_partitioned_call(\n\u001b[1;32m   1082\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_proto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1083\u001b[0;31m           executor_type=executor_type)\n\u001b[0m\u001b[1;32m   1084\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m       outputs = gen_functional_ops.partitioned_call(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, message, code)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Exception class to handle not ok Status.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "# este es el modelo estándar, con la entrada y la salida esperadas\n",
    "# hay que sustituirlo por un iterador.\n",
    "\n",
    "\n",
    "# No olvidarse de incluir un train_history para recuperar los datos del entrenamiento\n",
    "# 5000 epochs es una locura y el número de steps per epoch es muy bajo (no se recorren todas las imagenes)\n",
    "# el mínimo steps_per_epoch debería de ser total_muestras/batch_size\n",
    "\n",
    "lt = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "model_file = \"/mnt/DATA-SSD/DataSandbox/model{epoch:04d}.h5\"\n",
    "'''\n",
    "# with callback\n",
    "train_history = model.fit(train_iterator,\n",
    "                          epochs=500,\n",
    "                          steps_per_epoch=4000,\n",
    "                          verbose=1,\n",
    "                          callbacks = [tf.keras.callbacks.ModelCheckpoint(model_file, verbose=1)])\n",
    "'''\n",
    "\n",
    "# with no callback\n",
    "train_history = model.fit(train_iterator,\n",
    "                          epochs=500,\n",
    "                          steps_per_epoch=4000,\n",
    "                          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
